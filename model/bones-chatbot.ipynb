{
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 6967038,
     "sourceType": "datasetVersion",
     "datasetId": 4002732
    }
   ],
   "dockerImageVersionId": 30580,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, Concatenate, Lambda\n",
    "np.random.seed(23)\n",
    "random.seed(23)\n",
    "snapshot_folder = '/kaggle/working/'\n",
    "data_path = '/kaggle/input/cornell-movie-dialog/'\n",
    "\n",
    "# assert os.path.isdir(snapshot_folder) == True\n",
    "# assert os.path.isdir(data_path) == True\n",
    "print(tf.__version__)\n",
    "\n",
    "test = False\n",
    "print('test: ', test)\n",
    "if test:\n",
    "    GRU_units = 10\n",
    "    batch_size = 4\n",
    "    emb_dim = 10\n",
    "else:\n",
    "    GRU_units = 256\n",
    "    batch_size = 32\n",
    "    emb_dim = 50\n",
    "\n",
    "init_lr = 0.0005"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:39:31.705479Z",
     "iopub.execute_input": "2023-11-14T15:39:31.705887Z",
     "iopub.status.idle": "2023-11-14T15:39:45.310926Z",
     "shell.execute_reply.started": "2023-11-14T15:39:31.705857Z",
     "shell.execute_reply": "2023-11-14T15:39:45.309970Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def progressBar(value, endvalue, bar_length=20, job='Job'):\n",
    "\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\r{0} Completion: [{1}] {2}%\".format(job,arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def print_tensor(t):\n",
    "    print(K.get_value(t))\n",
    "    \n",
    "def to_tensor(t):\n",
    "    return tf.convert_to_tensor(t)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:39:45.312807Z",
     "iopub.execute_input": "2023-11-14T15:39:45.313709Z",
     "iopub.status.idle": "2023-11-14T15:39:45.320600Z",
     "shell.execute_reply.started": "2023-11-14T15:39:45.313664Z",
     "shell.execute_reply": "2023-11-14T15:39:45.319567Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reading the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "lines = open(data_path+'movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open(data_path+'movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n') # index of related lines\n",
    "\n",
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "        \n",
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = [ ]\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))\n",
    "\n",
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "pairs = []\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        pairs.append([id2line[conv[i]],id2line[conv[i+1]]])\n",
    "        \n",
    "# Check if we have loaded the data correctly\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(pairs[i][0])\n",
    "    print(pairs[i][1])\n",
    "    print()\n",
    "    \n",
    "len(pairs)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:39:45.321915Z",
     "iopub.execute_input": "2023-11-14T15:39:45.322254Z",
     "iopub.status.idle": "2023-11-14T15:39:47.110255Z",
     "shell.execute_reply.started": "2023-11-14T15:39:45.322222Z",
     "shell.execute_reply": "2023-11-14T15:39:47.109176Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# replacing many abbreviations and lower casing the words\n",
    "def replace_phrase(pairs):\n",
    "    p = pairs.copy()\n",
    "\n",
    "    for i in p:\n",
    "        for j in range(0,2):\n",
    "            i[j] = i[j].lower()\n",
    "            i[j] = re.sub(r\"there's\", \"there is\", i[j])\n",
    "            i[j] = re.sub(r\"i'm\", \"i am\", i[j])\n",
    "            i[j] = re.sub(r\"he's\", \"he is\", i[j])\n",
    "            i[j] = re.sub(r\"she's\", \"she is\", i[j])\n",
    "            i[j] = re.sub(r\"it's\", \"it is\", i[j])\n",
    "            i[j] = re.sub(r\"that's\", \"that is\", i[j])\n",
    "            i[j] = re.sub(r\"what's\", \"that is\", i[j])\n",
    "            i[j] = re.sub(r\"where's\", \"where is\", i[j])\n",
    "            i[j] = re.sub(r\"how's\", \"how is\", i[j])\n",
    "            i[j] = re.sub(r\"\\'ll\", \" will\", i[j])\n",
    "            i[j] = re.sub(r\"\\'ve\", \" have\", i[j])\n",
    "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
    "            i[j] = re.sub(r\"\\'d\", \" would\", i[j])\n",
    "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
    "            i[j] = re.sub(r\"won't\", \"will not\", i[j])\n",
    "            i[j] = re.sub(r\"can't\", \"cannot\", i[j])\n",
    "            i[j] = re.sub(r\"n't\", \" not\", i[j])\n",
    "            i[j] = re.sub(r\"n'\", \"ng\", i[j])\n",
    "            i[j] = re.sub(r\"'bout\", \"about\", i[j])\n",
    "            i[j] = re.sub(r\"'til\", \"until\", i[j])\n",
    "            i[j] = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", i[j])\n",
    "            i[j] = i[j].strip()\n",
    "    return p\n",
    "replaced_pairs = replace_phrase(pairs)\n",
    "replaced_pairs[:5]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:39:47.112382Z",
     "iopub.execute_input": "2023-11-14T15:39:47.112693Z",
     "iopub.status.idle": "2023-11-14T15:40:01.698498Z",
     "shell.execute_reply.started": "2023-11-14T15:39:47.112668Z",
     "shell.execute_reply": "2023-11-14T15:40:01.697599Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_data(pairs):\n",
    "    p = pairs.copy()\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for i in p:\n",
    "        # tokenize\n",
    "        i[0], i[1] = i[0].split(), i[1].split()\n",
    "        # convert to lower case\n",
    "        i[0], i[1] = [word.lower() for word in i[0]], [word.lower() for word in i[1]]\n",
    "        # remove punctuation from each token\n",
    "        i[0], i[1] = [w.translate(table) for w in i[0]], [w.translate(table) for w in i[1]]\n",
    "        # remove tokens with numbers in them\n",
    "        i[0], i[1] = [word for word in i[0] if word.isalpha()], [word for word in i[1] if word.isalpha()]\n",
    "        # store as string\n",
    "        i[0], i[1] =  ' '.join(i[0]), ' '.join(i[1])\n",
    "            \n",
    "    return p\n",
    "\n",
    "clean_pairs = clean_data(replaced_pairs)\n",
    "clean_pairs[:5]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:40:01.699837Z",
     "iopub.execute_input": "2023-11-14T15:40:01.700541Z",
     "iopub.status.idle": "2023-11-14T15:40:06.096250Z",
     "shell.execute_reply.started": "2023-11-14T15:40:01.700505Z",
     "shell.execute_reply": "2023-11-14T15:40:06.095187Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# adding the start and end token to our sentences\n",
    "start_token = '<startseq>'\n",
    "end_token = '<endseq>'\n",
    "\n",
    "def add_end_start_tokens(pairs):\n",
    "    p = pairs.copy()\n",
    "    for i in p:\n",
    "        i[0] = start_token + ' '  + i[0] + ' ' + end_token\n",
    "        i[1] = start_token + ' '  + i[1] + ' ' + end_token\n",
    "    return p\n",
    "\n",
    "tokenized_pairs = add_end_start_tokens(clean_pairs)\n",
    "tokenized_pairs[:5]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:40:06.097247Z",
     "iopub.execute_input": "2023-11-14T15:40:06.097534Z",
     "iopub.status.idle": "2023-11-14T15:40:06.325026Z",
     "shell.execute_reply.started": "2023-11-14T15:40:06.097512Z",
     "shell.execute_reply": "2023-11-14T15:40:06.324094Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# finding the maximum length of questions and answers\n",
    "# because there are senteces with unusually long lengths, we caculate the max length that 80% of data can be placed in\n",
    "def max_length(pairs,prct):\n",
    "    # Create a list of all the captions\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for i in pairs:\n",
    "        questions.append(i[0])\n",
    "        answers.append(i[1])\n",
    "        \n",
    "    length_questions = list(len(d.split()) for d in questions)\n",
    "    length_answers = list(len(d.split()) for d in answers)\n",
    "\n",
    "    print('percentile {} of len of questions: {}'.format(prct,np.percentile(length_questions, prct)))\n",
    "    print('longest sentence: ', max(length_questions))\n",
    "    print()\n",
    "    print('percentile {} of len of answers: {}'.format(prct,np.percentile(length_answers, prct)))\n",
    "    print('longest sentence: ', max(length_answers))\n",
    "    print()\n",
    "    return int(np.percentile(length_questions, prct)),int(np.percentile(length_answers, prct))\n",
    "\n",
    "max_len_q,max_len_a = max_length(tokenized_pairs,80)\n",
    "\n",
    "print('max-len questions for training: ', max_len_q)\n",
    "print('max-len answers for training: ', max_len_a)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:40:06.326154Z",
     "iopub.execute_input": "2023-11-14T15:40:06.326433Z",
     "iopub.status.idle": "2023-11-14T15:40:06.872738Z",
     "shell.execute_reply.started": "2023-11-14T15:40:06.326410Z",
     "shell.execute_reply": "2023-11-14T15:40:06.871796Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove questions and answers that are shorter than 2 words and longer than maxlen.\n",
    "min_line_len = 2 # two words are for tokens\n",
    "\n",
    "def set_length(tokenized_pairs):\n",
    "    pairs_final = []\n",
    "    for p in tokenized_pairs:\n",
    "        if (\n",
    "            len(p[0].split())>=min_line_len and len(p[1].split())>=min_line_len \n",
    "           and len(p[0].split())<=max_len_q and len(p[1].split())<=max_len_a):\n",
    "                \n",
    "            pairs_final.append(p)\n",
    "            \n",
    "    return pairs_final\n",
    "\n",
    "pairs_final = set_length(tokenized_pairs)\n",
    "len(pairs_final)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:40:06.873990Z",
     "iopub.execute_input": "2023-11-14T15:40:06.874329Z",
     "iopub.status.idle": "2023-11-14T15:40:07.622724Z",
     "shell.execute_reply.started": "2023-11-14T15:40:06.874301Z",
     "shell.execute_reply": "2023-11-14T15:40:07.621841Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating the vocabulary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# making a vocabulary of the words that occur more than word_count_threshold time\n",
    "def create_reoccurring_vocab(pairs, word_count_threshold = 5):\n",
    "    p = pairs\n",
    "    # Create a list of all the captions\n",
    "    all_captions = []\n",
    "    for i in p:\n",
    "        for j in i:\n",
    "            all_captions.append(j)\n",
    "\n",
    "    # Consider only words which occur at least 10 times in the corpus\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    vocab = list(set(vocab))\n",
    "    print('Short vocab size: %d ' % len(vocab))\n",
    "    return vocab\n",
    "\n",
    "# each word in the vocabulary must be used in the data atleast 20 times\n",
    "short_vocab = create_reoccurring_vocab(pairs_final, word_count_threshold = 4)\n",
    "# removing one character words from vocab except for 'a'\n",
    "for v in short_vocab:\n",
    "    if len(v) == 1 and v!='a' and v!='i':\n",
    "        short_vocab.remove(v) \n",
    "\n",
    "short_vocab = sorted(short_vocab)[1:]\n",
    "short_vocab[:5]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:40:07.624278Z",
     "iopub.execute_input": "2023-11-14T15:40:07.624649Z",
     "iopub.status.idle": "2023-11-14T15:40:08.585715Z",
     "shell.execute_reply.started": "2023-11-14T15:40:07.624616Z",
     "shell.execute_reply": "2023-11-14T15:40:08.584721Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_len = len(short_vocab) + 1 # since index 0 is used as padding, we have to increase the vocab size\n",
    "vocab_len"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:40:08.589375Z",
     "iopub.execute_input": "2023-11-14T15:40:08.589655Z",
     "iopub.status.idle": "2023-11-14T15:40:08.595425Z",
     "shell.execute_reply.started": "2023-11-14T15:40:08.589633Z",
     "shell.execute_reply": "2023-11-14T15:40:08.594483Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# keep the pairs that have the words in vocab\n",
    "def trimRareWords(voc, pairs):\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    i=0\n",
    "    for pair in pairs:\n",
    "        i+=1\n",
    "        progressBar(value=i,endvalue=len(pairs))\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"\\nTrimmed from {} pairs to {}\".format(len(pairs), len(keep_pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# # Trim voc and pairs\n",
    "pairs_final = trimRareWords(short_vocab, pairs_final)\n",
    "with open ('final_pairs_v21.pkl','wb') as f:\n",
    "    pairs_final = pickle.dump(pairs_final,f)\n",
    "    \n",
    "with open ('final_pairs_v21.pkl','rb') as f:\n",
    "    pairs_final = pickle.load(f)\n",
    "    \n",
    "pairs_final_train = pairs_final\n",
    "len(pairs_final_train)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-14T15:40:08.596701Z",
     "iopub.execute_input": "2023-11-14T15:40:08.596963Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Create an instance of the tokenizer object:\n",
    "tokenizer = Tokenizer(filters = [])\n",
    "tokenizer.fit_on_texts(short_vocab)\n",
    "\n",
    "ixtoword = {} # index to word dic\n",
    "wordtoix = tokenizer.word_index # word to index dic\n",
    "pad_token = 'pad0'\n",
    "ixtoword[0] = pad_token # no word in vocab has index 0. but padding is indicated with 0\n",
    "\n",
    "for w in tokenizer.word_index:\n",
    "    ixtoword[tokenizer.word_index[w]] = w"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### making the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    " # Making the embedding mtrix\n",
    "def make_embedding_layer(embedding_dim=100, glove=True):\n",
    "    if glove == False:\n",
    "        print('Just a zero matrix loaded')\n",
    "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \n",
    "    else:\n",
    "        print('Loading glove...')\n",
    "        glove_dir = '/kaggle/working/'\n",
    "        embeddings_index = {} \n",
    "        f = open(os.path.join(glove_dir, 'glove.6B.'+str(embedding_dim)+'d.txt'), encoding=\"utf-8\")\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print(\"GloVe \",embedding_dim, ' loded!')\n",
    "        # Get 200-dim dense vector for each of the vocab_rocc\n",
    "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
    "        for word, i in wordtoix.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found in the embedding index will be all zeros\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we \n",
    "                                                                                           # do not train the embedding layer\n",
    "                                                                                           # we use 0 as padding so => mask_zero=True\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([embedding_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "embeddings = make_embedding_layer(embedding_dim=emb_dim, glove=not test)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.Bidirectional1 = Bidirectional(GRU(enc_units, \n",
    "                                                return_sequences=True, \n",
    "                                                return_state=True,\n",
    "                                                recurrent_initializer='glorot_uniform',\n",
    "                                                name='gru_1'), name='bidirectional_encoder1')\n",
    "                                                                                                \n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.Inp = Input(shape=(max_len_q,)) # size of questions\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        output, state_f, state_b = self.Bidirectional1(x)\n",
    "\n",
    "        return output, state_f, state_b\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ],
   "metadata": {
    "id": "nZ2rI24i3jFg",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoder = Encoder(emb_dim, GRU_units)"
   ],
   "metadata": {
    "id": "60gSVh05Jl6l",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.units = units\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        \n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ],
   "metadata": {
    "id": "umohpBN2OM94",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_size\n",
    "        self.embeddings = embeddings\n",
    "        self.units = 2 * dec_units # because we use bidirectional encoder\n",
    "        self.fc = Dense(vocab_len, activation='softmax', name='dense_layer')\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        self.decoder_gru_l1 = GRU(self.units, return_sequences=True, \n",
    "                                  return_state= False, recurrent_initializer='glorot_uniform' ,name='decoder_gru1')\n",
    "        self.decoder_gru_l2 = GRU(self.units, return_sequences=False, \n",
    "                                  return_state= True, recurrent_initializer='glorot_uniform' ,name='decoder_gru2') \n",
    "        self.dropout = Dropout(0.4)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # concat input and context vector together\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        x = self.decoder_gru_l1(x)\n",
    "        x = self.dropout(x)\n",
    "        output, state = self.decoder_gru_l2(x)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ],
   "metadata": {
    "id": "yJ_B3mhW3jFk",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "decoder = Decoder(vocab_len, emb_dim, GRU_units)"
   ],
   "metadata": {
    "id": "P5UY8wko3jFp",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replying questions\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ],
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import unicodedata\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(sentence):\n",
    "    \n",
    "    attention_plot = np.zeros((max_len_a, max_len_q))\n",
    "\n",
    "    sentence = unicode_to_ascii(sentence.lower())\n",
    "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
    "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
    "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, GRU_units))]\n",
    "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "    dec_input = tf.expand_dims([wordtoix[start_token]], 1)\n",
    "\n",
    "    for t in range(max_len_a):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = K.get_value(attention_weights)\n",
    "        \n",
    "        predicted_id =  K.get_value(tf.argmax(predictions[0]))       \n",
    "\n",
    "        if ixtoword[predicted_id] == end_token:\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        result += ixtoword[predicted_id] + ' '\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ],
   "metadata": {
    "id": "EbQpyYs13jF_",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "s5hQWlbN3jGF",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def answer(sentence, training=False):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    if training:\n",
    "        return result\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted answer: {}'.format(result))\n",
    "    attention_plot = attention_plot[1:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' ')[:-1])"
   ],
   "metadata": {
    "id": "sl9zUHzg3jGI",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def beam_search(sentence, k=3, maxsample=max_len_a, use_unk=False, oov=None, eos=end_token):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    \n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [[wordtoix[start_token]]]\n",
    "    live_scores = [0]\n",
    "\n",
    "    sentence = unicode_to_ascii(sentence.lower())\n",
    "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
    "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
    "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    hidden = [tf.zeros((1, GRU_units))]\n",
    "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "    dec_input = tf.expand_dims([wordtoix[start_token]], 0)\n",
    "        \n",
    "    while live_k and dead_k < k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
    "        probs = K.get_value(predictions[0])\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        cand_flat = cand_scores.flatten()\n",
    "\n",
    "        # find the best (lowest) scores we have from all possible samples and new words\n",
    "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "        live_scores = cand_flat[ranks_flat]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = vocab_len\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    final_samples = dead_samples + live_samples\n",
    "    final_scores = dead_scores + live_scores   \n",
    "    \n",
    "    # cutting the strong where end_token is encounterd\n",
    "    for i in range(len(final_scores)):\n",
    "        final_scores[i] /= len(final_samples[i]) # normalizing the scores\n",
    "    \n",
    "    final_result =[]\n",
    "    \n",
    "    for i in range(len(final_scores)):\n",
    "        final_result.append((final_scores[i],final_samples[i]))\n",
    "    \n",
    "    final_list_ix = max(final_result)[1]\n",
    "    final_list_word = [ixtoword[f] for f in final_list_ix]\n",
    "    final_sentence = ' '.join(final_list_word[1:])\n",
    "    end_ix = final_sentence.find(end_token)\n",
    "    return final_sentence[:end_ix]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the optimizer and the loss function"
   ],
   "metadata": {
    "id": "_ch_71VbIRfK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = tf.keras.optimizers.Adam(init_lr)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = K.sparse_categorical_crossentropy(real, pred, from_logits= False)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ],
   "metadata": {
    "id": "WmTHr5iV3jFr",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoints (Object-based saving)"
   ],
   "metadata": {
    "id": "DMVWzzsfNl4e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(snapshot_folder, str(emb_dim)+\"-ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ],
   "metadata": {
    "id": "Zj8bXQTgNwrF",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ],
   "metadata": {
    "id": "hpObfY22IddU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden_f, enc_hidden_b = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "        dec_input = tf.expand_dims([wordtoix[start_token]] * batch_size, 1) # dec_input initially == start_token\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            \n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions) # each time just use one timestep output\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1) # expected output at this time becomes input for next timestep\n",
    "            \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ],
   "metadata": {
    "id": "sC9ArXSsVfqn",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history={'loss':[]}\n",
    "smallest_loss = np.inf\n",
    "best_ep = 1\n",
    "EPOCHS = 150 # but 150 is enough\n",
    "enc_hidden = encoder.initialize_hidden_state()\n",
    "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
    "current_ep = 1"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test_bot(k = 3, beam = False):\n",
    "    print('#'*20)\n",
    "    q = 'Hello'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'How are you'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q= 'Are you my friend'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'What are you doing'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'What your favorite restaurant'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'Who are you'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'Do you want to go out'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('#'*20)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_history():\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(best_ep,smallest_loss,'ro')\n",
    "    plt.plot(history['loss'],'b-')\n",
    "    plt.legend(['best','loss'])\n",
    "    plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_loss = K.constant(0)\n",
    "X, y = [], []\n",
    "for ep in range(current_ep,EPOCHS):\n",
    "    current_ep = ep    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    btch = 1\n",
    "\n",
    "    for p in pairs_final_train:     \n",
    "        \n",
    "        question = p[0]\n",
    "        label = p[1]\n",
    "        # find the index of each word of the caption in vocabulary\n",
    "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
    "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
    "        # encoder input and decoder input and label\n",
    "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
    "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
    "        \n",
    "        X.append(enc_in_seq)\n",
    "        y.append(dec_out_seq)\n",
    "\n",
    "        if len(X) == batch_size :\n",
    "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "            X , y = [], []\n",
    "            btch += 1\n",
    "            if btch % (steps_per_epoch//6) == 0:\n",
    "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep , btch, K.get_value(batch_loss)))\n",
    "\n",
    "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
    "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep ,epoch_loss))\n",
    "    history['loss'].append(epoch_loss)\n",
    "    \n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    test_bot(k=5)\n",
    "\n",
    "    if epoch_loss < smallest_loss:\n",
    "        smallest_loss = epoch_loss\n",
    "        best_ep = ep \n",
    "        print('check point saved!')\n",
    "    \n",
    "    if ep % 3 == 0:\n",
    "        plot_history()\n",
    "        \n",
    "    print('Best epoch so far: ',best_ep,' smallest loss:',smallest_loss)\n",
    "    print('Time taken for the epoch {:.3f} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    print('=' * 40)   "
   ],
   "metadata": {
    "id": "ddefjBMa3jF0",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears that themodel is not getting better anymore, it is overfitting. so I stop the training here and I use the check point at epoch 82 as the inference model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plot_history()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "id": "n250XbnjOaqP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint.restore(snapshot_folder+'/'+str(emb_dim)+\"-ckpt-120\") "
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_bot()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q = \"I like you\"\n",
    "answer(q, training=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q = \"How do you answer\"\n",
    "answer(q, training=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q = \"are you hungry\"\n",
    "answer(q, training=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q = \"who are you\"\n",
    "answer(q, training=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q = \"Do you drink\"\n",
    "answer(q, training=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
